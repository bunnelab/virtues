{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "from umap import UMAP\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from safetensors import safe_open\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "from modules.multiplex_virtues import MultiplexVirtues\n",
    "from datasets.multiplex_dataset import MultiplexDataset\n",
    "from utils.utils import load_marker_embeddings\n",
    "from utils.cell_tokens import compute_cell_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell Phenotyping with Cell Summary Tokens\n",
    "In this notebook, we demonstrate how to use VirTues to compute cell summary tokens and apply them to cell phenotyping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Model Initialization\n",
    "\n",
    "To get started, instantiate the VirTues model and load its pretrained weights.\n",
    "\n",
    "A default configuration file is provided at `configs/base_config.yaml`. This file contains all parameters required for the released VirTues model.\n",
    "\n",
    "In addition, you must specify a directory containing the embeddings for all markers used. Each embedding should be saved as a `.pt` file, named according to its respective UniProt ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = OmegaConf.load('configs/base_config.yaml')\n",
    "\n",
    "PATH_MARKER_EMBEDDINGS = 'assets/example_dataset/marker_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_embeddings = load_marker_embeddings(PATH_MARKER_EMBEDDINGS)\n",
    "\n",
    "model = MultiplexVirtues(\n",
    "    use_default_config = False,\n",
    "    custom_config = None,\n",
    "    prior_bias_embeddings=marker_embeddings,\n",
    "    prior_bias_embedding_type='esm',\n",
    "    prior_bias_embedding_fusion_type='add',\n",
    "    patch_size=conf.model.patch_size,\n",
    "    model_dim=conf.model.model_dim,\n",
    "    feedforward_dim=conf.model.feedforward_dim,\n",
    "    encoder_pattern=conf.model.encoder_pattern,\n",
    "    num_encoder_heads=conf.model.num_encoder_heads,\n",
    "    decoder_pattern=conf.model.decoder_pattern,\n",
    "    num_decoder_heads=conf.model.num_decoder_heads,\n",
    "    num_hidden_layers=conf.model.num_decoder_hidden_layers,\n",
    "    positional_embedding_type=conf.model.positional_embedding_type,\n",
    "    dropout=conf.model.dropout,\n",
    "    group_layers=conf.model.group_layers,\n",
    "    norm_after_encoder_decoder=conf.model.norm_after_encoder_decoder,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide model weights of our pretrained VirTues instance on Hugging Face Hub. These can be downloaded via `hf_hub_download` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = 'assets/checkpoints'\n",
    "hf_hub_download(repo_id='bunnelab/virtues', filename='model.safetensors', local_dir=CACHE_DIR)\n",
    "\n",
    "weights = {}\n",
    "with safe_open(os.path.join(CACHE_DIR, 'model.safetensors'), framework=\"pt\", device='cpu') as f:\n",
    "    for k in f.keys():\n",
    "        weights[k] = f.get_tensor(k)\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dataset Initialization\n",
    "Next, let us instantiate a dataset. We provide a simple example dataset at `assets/example_dataset` consisting out of a single tissue image, which we can access using the class `MultiplexDataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_conf = OmegaConf.load('configs/datasets/example_config.yaml')['datasets']['example_dataset']\n",
    "\n",
    "dataset = MultiplexDataset(\n",
    "            tissue_dir=ds_conf.tissue_dir,\n",
    "            crop_dir=ds_conf.crop_dir,\n",
    "            mask_dir=ds_conf.mask_dir,\n",
    "            tissue_index=ds_conf.tissue_index,\n",
    "            crop_index=ds_conf.crop_index,\n",
    "            channels_file=ds_conf.channels_file,\n",
    "            quantiles_file=ds_conf.quantiles_file,\n",
    "            means_file=ds_conf.means_file,\n",
    "            stds_file=ds_conf.stds_file,\n",
    "            marker_embedding_dir=PATH_MARKER_EMBEDDINGS,\n",
    "            split='test',\n",
    "            crop_size=conf.data.crop_size,\n",
    "            patch_size=conf.model.patch_size,\n",
    "            masking_ratio=conf.data.masking_ratio,\n",
    "            channel_fraction=conf.data.channel_fraction,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dataset class, we can load a tissue image along with its corresponding cell-segmentation mask and the indices of the marker embeddings.\\\n",
    "These indices specify both the identity and the ordering of the markers present in the image, allowing the model to correctly interpret the measurement channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid = 'cords24_ocmzljpb_1'\n",
    "x = dataset.get_tissue(tid, preprocess=False)\n",
    "midxs = dataset.get_marker_indices()\n",
    "mask = dataset.get_segmentation_mask(tid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Computing Cell Summary Tokens\n",
    "We compute cell summary tokens by first embedding the entire image into patch-level summary tokens using VirTues. These patch tokens are then aggregated according to the segmentation mask, similiar to a convolution, to produce cell-level tokens.\n",
    "We provide the utility function `utils.cell_tokens.compute_cell_tokens` to perform this computation. Importantly, we recommend padding the image with zeros (corresponding to no signal) before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size = 120\n",
    "\n",
    "x = torch.nn.functional.pad(x, pad=(pad_size, pad_size, pad_size, pad_size), mode='constant', value=0)\n",
    "mask = torch.nn.functional.pad(mask, pad=(pad_size, pad_size, pad_size, pad_size), mode='constant', value=0)\n",
    "\n",
    "x = dataset._preprocess(tissue_id=tid, multiplex=x)\n",
    "\n",
    "cell_ids, cell_tokens, crop_tokens, indices = compute_cell_tokens(model, x, midxs, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. UMAP visualization of the cell tokens\n",
    "Let us visualize the structure of our cell tokens as as a 2D UMAP projection.\n",
    "For the sample tissue, we provide cell type annotations at `assets/example_dataset/sce_annotations.csv`. We can use these to color our UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um = UMAP(n_components=2, random_state=42)\n",
    "cell_tokens_2d = um.fit_transform(cell_tokens.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv('assets/example_dataset/sce_annotations.csv')\n",
    "annotations.set_index('cell_id', inplace=True)\n",
    "labels = annotations.loc[cell_ids]['cell_category_regen'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "sns.scatterplot(x=cell_tokens_2d[:,0], y=cell_tokens_2d[:,1], hue=labels, hue_order=['Tumor', 'Fibroblast', 'Immune', 'T cell', 'Vessel', 'Other'], palette='tab10', s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Cell Phenotyping using Logistic Regression\n",
    "Cell tokens can be leveraged for downstream cell phenotyping. A straightforward approach is to train a simple logistic regression model using the token representations as input features.\\\n",
    "For illustration, the example below uses a random trainâ€“test split of the loaded cells. In practice, however, these splits group-wise split by patient identities to avoid data leakage and ensure proper evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(cell_tokens.numpy(), labels, test_size=0.2, random_state=0, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model = lr_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, lr_model.predict(test_X), digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tissuevit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
