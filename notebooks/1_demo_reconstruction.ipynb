{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from safetensors import safe_open\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from modules.multiplex_virtues import MultiplexVirtues\n",
    "from datasets.multiplex_dataset import MultiplexDataset\n",
    "from utils.utils import load_marker_embeddings\n",
    "from utils.masking import generate_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction and Inpainting\n",
    "In this notebook, we demonstrate how to use VirTues to reconstruct partially or fully masked channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Model Initialization\n",
    "\n",
    "To get started, instantiate the VirTues model and load its pretrained weights.\n",
    "\n",
    "A default configuration file is provided at `configs/base_config.yaml`. This file contains all parameters required for the released VirTues model.\n",
    "\n",
    "In addition, you must specify a directory containing the embeddings for all markers used. Each embedding should be saved as a `.pt` file, named according to its respective UniProt ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = OmegaConf.load('configs/base_config.yaml')\n",
    "\n",
    "PATH_MARKER_EMBEDDINGS = 'assets/example_dataset/marker_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_embeddings = load_marker_embeddings(PATH_MARKER_EMBEDDINGS)\n",
    "\n",
    "model = MultiplexVirtues(\n",
    "    use_default_config = False,\n",
    "    custom_config = None,\n",
    "    prior_bias_embeddings=marker_embeddings,\n",
    "    prior_bias_embedding_type='esm',\n",
    "    prior_bias_embedding_fusion_type='add',\n",
    "    patch_size=conf.model.patch_size,\n",
    "    model_dim=conf.model.model_dim,\n",
    "    feedforward_dim=conf.model.feedforward_dim,\n",
    "    encoder_pattern=conf.model.encoder_pattern,\n",
    "    num_encoder_heads=conf.model.num_encoder_heads,\n",
    "    decoder_pattern=conf.model.decoder_pattern,\n",
    "    num_decoder_heads=conf.model.num_decoder_heads,\n",
    "    num_hidden_layers=conf.model.num_decoder_hidden_layers,\n",
    "    positional_embedding_type=conf.model.positional_embedding_type,\n",
    "    dropout=conf.model.dropout,\n",
    "    group_layers=conf.model.group_layers,\n",
    "    norm_after_encoder_decoder=conf.model.norm_after_encoder_decoder,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide model weights of our pretrained VirTues instance on Hugging Face Hub. These can be downloaded via `hf_hub_download` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = 'assets/checkpoints'\n",
    "hf_hub_download(repo_id='bunnelab/virtues', filename='model.safetensors', local_dir=CACHE_DIR)\n",
    "\n",
    "weights = {}\n",
    "with safe_open(os.path.join(CACHE_DIR, 'model.safetensors'), framework=\"pt\", device='cpu') as f:\n",
    "    for k in f.keys():\n",
    "        weights[k] = f.get_tensor(k)\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "model = model.cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dataset Initialization\n",
    "Next, let us instantiate a dataset. We provide a simple example dataset at `assets/example_dataset` consisting out of a single tissue image, which we can access using the class `MultiplexDataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_conf = OmegaConf.load('configs/datasets/example_config.yaml')['datasets']['example_dataset']\n",
    "\n",
    "dataset = MultiplexDataset(\n",
    "            tissue_dir=ds_conf.tissue_dir,\n",
    "            crop_dir=ds_conf.crop_dir,\n",
    "            mask_dir=ds_conf.mask_dir,\n",
    "            tissue_index=ds_conf.tissue_index,\n",
    "            crop_index=ds_conf.crop_index,\n",
    "            channels_file=ds_conf.channels_file,\n",
    "            quantiles_file=ds_conf.quantiles_file,\n",
    "            means_file=ds_conf.means_file,\n",
    "            stds_file=ds_conf.stds_file,\n",
    "            marker_embedding_dir=PATH_MARKER_EMBEDDINGS,\n",
    "            split='test',\n",
    "            crop_size=conf.data.crop_size,\n",
    "            patch_size=conf.model.patch_size,\n",
    "            masking_ratio=conf.data.masking_ratio,\n",
    "            channel_fraction=conf.data.channel_fraction,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dataset class, we can load a tissue image along with the indices of the marker embeddings.\\\n",
    "These indices specify both the identity and the ordering of the markers present in the image, allowing the model to correctly interpret the measurement channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.get_tissue('cords24_ocmzljpb_1')\n",
    "midxs = dataset.get_marker_indices()\n",
    "crop = x[:, 10:138, 10:138]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reconstructing a Partially Masked Channel\n",
    "We can reconstruct a masked image using a simple forward pass with VirTues. As a first step, we test reconstruction under independent partial masking applied to each channel. To generate these per-channel masks, you can use the helper function `utils.masking.generate_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = generate_mask(C=crop.shape[0], H=crop.shape[1]//8, W=crop.shape[2]//8, masking_ratio=(0.6,1.0))\n",
    "\n",
    "crop = crop.cuda()\n",
    "midx = midxs.cuda()\n",
    "mask = mask.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        output = model.forward([crop], [midxs], [mask])\n",
    "recon = output.decoded_multiplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_idx = 5\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "\n",
    "ax[0].imshow(crop[channel_idx].cpu().numpy(), cmap='inferno')\n",
    "for i in range(mask.shape[1]):\n",
    "    for j in range(mask.shape[2]):\n",
    "        if mask[channel_idx,i,j]:\n",
    "            ax[0].add_patch(plt.Rectangle((j*8, i*8), 8, 8, color='white'))\n",
    "ax[0].set_title('Masked Input')\n",
    "\n",
    "ax[1].imshow(recon[0][channel_idx].cpu().numpy(), cmap='inferno')\n",
    "ax[1].set_title('Reconstruction')\n",
    "ax[2].imshow(crop[channel_idx].cpu().numpy(), cmap='inferno')\n",
    "ax[2].set_title('Original Crop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reconstructing a Fully Masked Channel\n",
    "VirTues also supports inpainting of fully masked channels. To do this, simply generate a 3D mask where the channel to be inpainted is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_idx = 3\n",
    "mask = torch.zeros_like(mask)\n",
    "mask[channel_idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        output = model.forward([crop], [midxs], [mask])\n",
    "recon = output.decoded_multiplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "\n",
    "ax[0].imshow(crop[channel_idx].cpu().numpy(), cmap='inferno')\n",
    "for i in range(mask.shape[1]):\n",
    "    for j in range(mask.shape[2]):\n",
    "        if mask[channel_idx,i,j]:\n",
    "            ax[0].add_patch(plt.Rectangle((j*8, i*8), 8, 8, color='white'))\n",
    "ax[0].set_title('Masked Input')\n",
    "\n",
    "ax[1].imshow(recon[0][channel_idx].cpu().numpy(), cmap='inferno')\n",
    "ax[1].set_title('Reconstruction')\n",
    "ax[2].imshow(crop[channel_idx].cpu().numpy(), cmap='inferno')\n",
    "ax[2].set_title('Original Crop')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tissuevit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
